{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b08c9a0c",
   "metadata": {},
   "source": [
    "# Multilingual scoring\n",
    "\n",
    "Score verified Reddit posts using multilingual sentiment (XLM-RoBERTa) and a simple engagement metric."
   ]
  },
  {
   "cell_type": "code",
   "id": "a5b37eaa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T19:24:31.922390Z",
     "start_time": "2025-11-28T19:24:27.444807Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import duckdb\n",
    "from pathlib import Path\n",
    "from transformers import pipeline\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ivantyshchenko/Projects/Python/DataPatron/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "741bf3e6",
   "metadata": {},
   "source": [
    "## Load data from SQL\n",
    "\n",
    "Query verified submissions plus their comments directly into pandas using DuckDB."
   ]
  },
  {
   "metadata": {
    "SqlCellData": {
     "data_source_name": "reddit.db",
     "variableName$1": "df_sql1"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "%%sql\n",
    "-- Ð¡Ñ‚Ð²Ð¾Ñ€ÐµÐ½Ð½Ñ Ð½Ð¾Ð²Ð¾Ñ— Ð°Ð½Ð°Ð»Ñ–Ñ‚Ð¸Ñ‡Ð½Ð¾Ñ— Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ñ–\n",
    "CREATE TABLE main.dataset_posts_with_comments AS\n",
    "SELECT\n",
    "    s.id AS post_id,\n",
    "    s.author AS post_author,\n",
    "    s.created_at AS post_created_at,\n",
    "    s.subreddit,\n",
    "    s.title,\n",
    "    s.selftext,\n",
    "    s.score AS post_score,\n",
    "    s.upvote_ratio,\n",
    "    s.num_comments,\n",
    "    s.url,\n",
    "    s.domain,\n",
    "    ARRAY_AGG(c.body) FILTER (WHERE c.body IS NOT NULL) AS comments_content,\n",
    "    ARRAY_AGG(c.score) FILTER (WHERE c.score IS NOT NULL) AS comments_scores\n",
    "FROM\n",
    "    main.submissions_final_verified s\n",
    "LEFT JOIN\n",
    "    main.comments c ON s.id = c.submission_id\n",
    "GROUP BY\n",
    "    s.id,\n",
    "    s.author,\n",
    "    s.created_at,\n",
    "    s.subreddit,\n",
    "    s.title,\n",
    "    s.selftext,\n",
    "    s.score,\n",
    "    s.upvote_ratio,\n",
    "    s.num_comments,\n",
    "    s.url,\n",
    "    s.domain;"
   ],
   "id": "97c088826d2eaadc"
  },
  {
   "cell_type": "code",
   "id": "b0603ddf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T19:24:51.232409Z",
     "start_time": "2025-11-28T19:24:50.972307Z"
    }
   },
   "source": [
    "DB_PATH = Path('/Users/ivantyshchenko/Projects/Python/DataPatron/data/reddit.db')\n",
    "\n",
    "QUERY = '''\n",
    "SELECT * FROM dataset_posts_with_comments\n",
    "'''\n",
    "\n",
    "with duckdb.connect(str(DB_PATH)) as conn:\n",
    "    posts_df = conn.execute(QUERY).df()\n",
    "\n",
    "print(f'Loaded {len(posts_df)} posts')\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3702 posts\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "b6a607b3",
   "metadata": {},
   "source": [
    "## Set up multilingual sentiment model\n",
    "\n",
    "XLM-RoBERTa sentiment model with full-score output for downstream scoring."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T20:01:24.392066Z",
     "start_time": "2025-11-28T20:01:16.155936Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoConfig\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "\n",
    "def preprocess(text):\n",
    "  parts = []\n",
    "  for t in text.split(\" \"):\n",
    "      if t.startswith(\"@\") and len(t) > 1:\n",
    "          t = \"@user\"\n",
    "      if t.startswith(\"http\"):\n",
    "          t = \"http\"\n",
    "      parts.append(t)\n",
    "  return \" \".join(parts)\n",
    "\n",
    "model_name = \"cardiffnlp/twitter-xlm-roberta-base-sentiment\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "text = preprocess(\"Good night ðŸ˜Š\")\n",
    "encoded = tokenizer(text, return_tensors=\"pt\")\n",
    "scores = model(**encoded).logits[0].detach().numpy()\n",
    "scores = softmax(scores)\n",
    "\n",
    "for idx in scores.argsort()[::-1]:\n",
    "  print(f\"{config.id2label[idx]} {scores[idx]:.4f}\")"
   ],
   "id": "a9994a5aa4601690",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ivantyshchenko/Projects/Python/DataPatron/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'endswith'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mAttributeError\u001B[39m                            Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[2]\u001B[39m\u001B[32m, line 16\u001B[39m\n\u001B[32m     13\u001B[39m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33m \u001B[39m\u001B[33m\"\u001B[39m.join(parts)\n\u001B[32m     15\u001B[39m model_name = \u001B[33m\"\u001B[39m\u001B[33mcardiffnlp/twitter-xlm-roberta-base-sentiment\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m---> \u001B[39m\u001B[32m16\u001B[39m tokenizer = \u001B[43mAutoTokenizer\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     17\u001B[39m config = AutoConfig.from_pretrained(model_name)\n\u001B[32m     18\u001B[39m model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Projects/Python/DataPatron/.venv/lib/python3.12/site-packages/transformers/models/auto/tokenization_auto.py:1175\u001B[39m, in \u001B[36mAutoTokenizer.from_pretrained\u001B[39m\u001B[34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001B[39m\n\u001B[32m   1172\u001B[39m tokenizer_class_py, tokenizer_class_fast = TOKENIZER_MAPPING[\u001B[38;5;28mtype\u001B[39m(config)]\n\u001B[32m   1174\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m tokenizer_class_fast \u001B[38;5;129;01mand\u001B[39;00m (use_fast \u001B[38;5;129;01mor\u001B[39;00m tokenizer_class_py \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[32m-> \u001B[39m\u001B[32m1175\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtokenizer_class_fast\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpretrained_model_name_or_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1176\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1177\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m tokenizer_class_py \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Projects/Python/DataPatron/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2113\u001B[39m, in \u001B[36mPreTrainedTokenizerBase.from_pretrained\u001B[39m\u001B[34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001B[39m\n\u001B[32m   2110\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   2111\u001B[39m         logger.info(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mloading file \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfile_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m from cache at \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresolved_vocab_files[file_id]\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m-> \u001B[39m\u001B[32m2113\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mcls\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_from_pretrained\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   2114\u001B[39m \u001B[43m    \u001B[49m\u001B[43mresolved_vocab_files\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2115\u001B[39m \u001B[43m    \u001B[49m\u001B[43mpretrained_model_name_or_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2116\u001B[39m \u001B[43m    \u001B[49m\u001B[43minit_configuration\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2117\u001B[39m \u001B[43m    \u001B[49m\u001B[43m*\u001B[49m\u001B[43minit_inputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2118\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtoken\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtoken\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2119\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2120\u001B[39m \u001B[43m    \u001B[49m\u001B[43mlocal_files_only\u001B[49m\u001B[43m=\u001B[49m\u001B[43mlocal_files_only\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2121\u001B[39m \u001B[43m    \u001B[49m\u001B[43m_commit_hash\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcommit_hash\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2122\u001B[39m \u001B[43m    \u001B[49m\u001B[43m_is_local\u001B[49m\u001B[43m=\u001B[49m\u001B[43mis_local\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2123\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtrust_remote_code\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtrust_remote_code\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2124\u001B[39m \u001B[43m    \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2125\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Projects/Python/DataPatron/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2359\u001B[39m, in \u001B[36mPreTrainedTokenizerBase._from_pretrained\u001B[39m\u001B[34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001B[39m\n\u001B[32m   2357\u001B[39m \u001B[38;5;66;03m# Instantiate the tokenizer.\u001B[39;00m\n\u001B[32m   2358\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m2359\u001B[39m     tokenizer = \u001B[38;5;28;43mcls\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43minit_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43minit_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   2360\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m import_protobuf_decode_error():\n\u001B[32m   2361\u001B[39m     logger.info(\n\u001B[32m   2362\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mUnable to load tokenizer model from SPM, loading from TikToken will be attempted instead.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   2363\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33m(Google protobuf error: Tried to load SPM model with non-SPM vocab file).\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m   2364\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Projects/Python/DataPatron/.venv/lib/python3.12/site-packages/transformers/models/xlm_roberta/tokenization_xlm_roberta_fast.py:108\u001B[39m, in \u001B[36mXLMRobertaTokenizerFast.__init__\u001B[39m\u001B[34m(self, vocab_file, tokenizer_file, bos_token, eos_token, sep_token, cls_token, unk_token, pad_token, mask_token, **kwargs)\u001B[39m\n\u001B[32m     92\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__init__\u001B[39m(\n\u001B[32m     93\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m     94\u001B[39m     vocab_file=\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m   (...)\u001B[39m\u001B[32m    104\u001B[39m ):\n\u001B[32m    105\u001B[39m     \u001B[38;5;66;03m# Mask token behave like a normal word, i.e. include the space before it\u001B[39;00m\n\u001B[32m    106\u001B[39m     mask_token = AddedToken(mask_token, lstrip=\u001B[38;5;28;01mTrue\u001B[39;00m, rstrip=\u001B[38;5;28;01mFalse\u001B[39;00m) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(mask_token, \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m mask_token\n\u001B[32m--> \u001B[39m\u001B[32m108\u001B[39m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[34;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[32m    109\u001B[39m \u001B[43m        \u001B[49m\u001B[43mvocab_file\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    110\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtokenizer_file\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtokenizer_file\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    111\u001B[39m \u001B[43m        \u001B[49m\u001B[43mbos_token\u001B[49m\u001B[43m=\u001B[49m\u001B[43mbos_token\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    112\u001B[39m \u001B[43m        \u001B[49m\u001B[43meos_token\u001B[49m\u001B[43m=\u001B[49m\u001B[43meos_token\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    113\u001B[39m \u001B[43m        \u001B[49m\u001B[43msep_token\u001B[49m\u001B[43m=\u001B[49m\u001B[43msep_token\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    114\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcls_token\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcls_token\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    115\u001B[39m \u001B[43m        \u001B[49m\u001B[43munk_token\u001B[49m\u001B[43m=\u001B[49m\u001B[43munk_token\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    116\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpad_token\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpad_token\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    117\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmask_token\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmask_token\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    118\u001B[39m \u001B[43m        \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    119\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    121\u001B[39m     \u001B[38;5;28mself\u001B[39m.vocab_file = vocab_file\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Projects/Python/DataPatron/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_fast.py:139\u001B[39m, in \u001B[36mPreTrainedTokenizerFast.__init__\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m    137\u001B[39m     \u001B[38;5;28mself\u001B[39m.vocab_file = kwargs.get(\u001B[33m\"\u001B[39m\u001B[33mvocab_file\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m    138\u001B[39m     \u001B[38;5;28mself\u001B[39m.additional_special_tokens = kwargs.get(\u001B[33m\"\u001B[39m\u001B[33madditional_special_tokens\u001B[39m\u001B[33m\"\u001B[39m, [])\n\u001B[32m--> \u001B[39m\u001B[32m139\u001B[39m     fast_tokenizer = \u001B[43mconvert_slow_tokenizer\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrom_tiktoken\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[32m    140\u001B[39m     slow_tokenizer = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    141\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Projects/Python/DataPatron/.venv/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:1857\u001B[39m, in \u001B[36mconvert_slow_tokenizer\u001B[39m\u001B[34m(transformer_tokenizer, from_tiktoken)\u001B[39m\n\u001B[32m   1855\u001B[39m     converter_class = SLOW_TO_FAST_CONVERTERS[tokenizer_class_name]\n\u001B[32m   1856\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m converter_class(transformer_tokenizer).converted()\n\u001B[32m-> \u001B[39m\u001B[32m1857\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m \u001B[43mtransformer_tokenizer\u001B[49m\u001B[43m.\u001B[49m\u001B[43mvocab_file\u001B[49m\u001B[43m.\u001B[49m\u001B[43mendswith\u001B[49m(\u001B[33m\"\u001B[39m\u001B[33mtekken.json\u001B[39m\u001B[33m\"\u001B[39m):\n\u001B[32m   1858\u001B[39m     transformer_tokenizer.original_tokenizer = transformer_tokenizer\n\u001B[32m   1859\u001B[39m     logger.info(\u001B[33m\"\u001B[39m\u001B[33mConverting from Mistral tekken.json\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[31mAttributeError\u001B[39m: 'NoneType' object has no attribute 'endswith'"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "bd78eaad",
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "end_time": "2025-11-28T20:00:53.803689Z",
     "start_time": "2025-11-28T19:59:08.394860Z"
    }
   },
   "source": [
    "MODEL_NAME = 'cardiffnlp/twitter-xlm-roberta-base-sentiment'\n",
    "\n",
    "sentiment_pipeline = pipeline(\n",
    "    'sentiment-analysis',\n",
    "    model=MODEL_NAME,\n",
    "    tokenizer=MODEL_NAME,\n",
    "    top_k=None,\n",
    "    truncation=True,\n",
    "    max_length=512\n",
    ")\n",
    "\n",
    "TOXIC_KEYWORDS = {\n",
    "    'fake', 'bot', 'propaganda', 'liar', 'stupid', 'idiot', 'scam',\n",
    "    'Ñ„ÐµÐ¹Ðº', 'Ð±Ð¾Ñ‚', 'Ð±Ñ€ÐµÑ…Ð½Ñ', 'Ð¿Ñ€Ð¾Ð¿Ð°Ð³Ð°Ð½Ð´Ð°', 'Ñ–Ð¿ÑÐ¾', 'Ð´ÑƒÑ€ÐµÐ½ÑŒ', 'Ñ–Ð´Ñ–Ð¾Ñ‚',\n",
    "    'Ð²Ñ€Ð°Ð½ÑŒÐµ', 'Ð»Ð¾Ð¶ÑŒ'\n",
    "}\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a15b6513",
   "metadata": {},
   "source": [
    "## Metric helpers\n",
    "\n",
    "Plain functions for multilingual sentiment scoring, efficiency, and UCQS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31f2e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_multilingual_score(text: str) -> float:\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return 0.0\n",
    "    try:\n",
    "        results = sentiment_pipeline(text)[0]\n",
    "        scores = {item['label'].lower(): item['score'] for item in results}\n",
    "        return scores.get('positive', 0.0) - scores.get('negative', 0.0)\n",
    "    except Exception as exc:\n",
    "        print(f'Sentiment fallback for text due to error: {exc}')\n",
    "        return 0.0\n",
    "\n",
    "def calculate_efficiency(post_score, num_comments, upvote_ratio) -> float:\n",
    "    score = post_score if pd.notnull(post_score) else 0\n",
    "    comments = num_comments if pd.notnull(num_comments) else 0\n",
    "    ratio = upvote_ratio if pd.notnull(upvote_ratio) else 0.5\n",
    "    raw_engagement = score + (comments * 2.0)\n",
    "    log_engagement = np.log1p(raw_engagement)\n",
    "    return round(log_engagement * ratio, 2)\n",
    "\n",
    "def calculate_ucqs(comments_content) -> float:\n",
    "    if not isinstance(comments_content, list) or not comments_content:\n",
    "        return 50.0\n",
    "    sentiments = []\n",
    "    toxic_count = 0\n",
    "    valid_comments = 0\n",
    "    for text in comments_content:\n",
    "        if not isinstance(text, str) or not text.strip():\n",
    "            continue\n",
    "        valid_comments += 1\n",
    "        score = to_multilingual_score(text[:512])\n",
    "        sentiments.append(score)\n",
    "        lower = text.lower()\n",
    "        if any(k in lower for k in TOXIC_KEYWORDS) or score < -0.7:\n",
    "            toxic_count += 1\n",
    "    if not sentiments:\n",
    "        return 50.0\n",
    "    s_avg = float(np.mean(sentiments))\n",
    "    s_var = float(np.std(sentiments))\n",
    "    t_ratio = toxic_count / valid_comments if valid_comments else 0\n",
    "    ucqs = 50 + (s_avg * 40) - (s_var * 20) - (t_ratio * 30)\n",
    "    return float(np.clip(ucqs, 0, 100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aba6ed1",
   "metadata": {},
   "source": [
    "## Apply metrics\n",
    "\n",
    "Compute both scores and inspect the result set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083710c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_df['efficiency'] = posts_df.apply(\n",
    "    lambda row: calculate_efficiency(row['post_score'], row['num_comments'], row['upvote_ratio']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "posts_df['ucqs'] = posts_df['comments_content'].apply(calculate_ucqs)\n",
    "\n",
    "posts_df[['post_id', 'efficiency', 'ucqs']].head()\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "f8a020c875c41846"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
