{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-29T13:30:44.184500Z",
     "start_time": "2025-11-29T13:30:39.908752Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import pipeline\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Plot styling configuration\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = [10, 6]\n",
    "\n",
    "print(\"✅ Libraries imported successfully.\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ivantyshchenko/Projects/Python/DataPatron/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Libraries imported successfully.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T13:32:00.687458Z",
     "start_time": "2025-11-29T13:32:00.679905Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class AdvancedMessageMetrics:\n",
    "    def __init__(self):\n",
    "        print(\"⏳ Loading Sentiment Model (XLM-RoBERTa)...\")\n",
    "        # Using CPU (device=-1). If you have a GPU, change to device=0\n",
    "        self.sentiment_pipe = pipeline(\n",
    "            \"sentiment-analysis\",\n",
    "            model=\"cardiffnlp/twitter-xlm-roberta-base-sentiment\",\n",
    "            tokenizer=\"cardiffnlp/twitter-xlm-roberta-base-sentiment\",\n",
    "            top_k=None,\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            device=\"mps\"\n",
    "        )\n",
    "\n",
    "        print(\"⏳ Loading Toxicity Model (Multilingual Toxic XLM-R)...\")\n",
    "        self.toxic_pipe = pipeline(\n",
    "            \"text-classification\",\n",
    "            model=\"unitary/multilingual-toxic-xlm-roberta\",\n",
    "            tokenizer=\"unitary/multilingual-toxic-xlm-roberta\",\n",
    "            top_k=None,\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            device=-1\n",
    "        )\n",
    "        print(\"✅ Models loaded!\")\n",
    "\n",
    "    def _get_sentiment_score(self, text):\n",
    "        \"\"\"Converts model output into a number from -1.0 (Negative) to 1.0 (Positive).\"\"\"\n",
    "        try:\n",
    "            results = self.sentiment_pipe(text)[0]\n",
    "            scores = {item['label'].lower(): item['score'] for item in results}\n",
    "\n",
    "            # Normalize label names (sometimes models return label_0/1/2)\n",
    "            if 'label_0' in scores:\n",
    "                scores['negative'] = scores.pop('label_0')\n",
    "                scores['neutral'] = scores.pop('label_1')\n",
    "                scores['positive'] = scores.pop('label_2')\n",
    "\n",
    "            # Formula: Positive minus Negative\n",
    "            return scores.get('positive', 0.0) - scores.get('negative', 0.0)\n",
    "        except Exception as e:\n",
    "            print(f\"Error in sentiment: {e}\")\n",
    "            return 0.0\n",
    "\n",
    "    def _get_toxicity_score(self, text):\n",
    "        \"\"\"Returns toxicity probability (0.0 - 1.0).\"\"\"\n",
    "        try:\n",
    "            results = self.toxic_pipe(text)[0]\n",
    "            scores = {item['label']: item['score'] for item in results}\n",
    "            # We take the general 'toxic' score\n",
    "            return scores.get('toxic', 0.0)\n",
    "        except Exception as e:\n",
    "            print(f\"Error in toxicity: {e}\")\n",
    "            return 0.0\n",
    "\n",
    "    def calculate_ucqs(self, comments_content):\n",
    "        \"\"\"\n",
    "        Calculates Unified Comment Quality Score (UCQS).\n",
    "        Input: List of comment texts or string representation of a list.\n",
    "        Output: Float 0-100.\n",
    "        \"\"\"\n",
    "        # 1. Validation and parsing of input data\n",
    "        if isinstance(comments_content, str):\n",
    "            try:\n",
    "                comments_content = ast.literal_eval(comments_content)\n",
    "            except:\n",
    "                return 50.0 # Return neutral baseline on parse error\n",
    "\n",
    "        if not isinstance(comments_content, list) or not comments_content:\n",
    "            return 50.0\n",
    "\n",
    "        sentiments = []\n",
    "        toxicity_scores = []\n",
    "\n",
    "        # 2. Analyze each comment\n",
    "        for text in comments_content:\n",
    "            if not isinstance(text, str) or not text.strip():\n",
    "                continue\n",
    "\n",
    "            # Truncate to 512 chars for speed\n",
    "            short_text = text[:512]\n",
    "\n",
    "            sentiments.append(self._get_sentiment_score(short_text))\n",
    "            toxicity_scores.append(self._get_toxicity_score(short_text))\n",
    "\n",
    "        if not sentiments:\n",
    "            return 50.0\n",
    "\n",
    "        # 3. Aggregating statistics\n",
    "        s_avg = np.mean(sentiments)      # Average sentiment (-1..1)\n",
    "        s_var = np.std(sentiments)       # Sentiment variance (0..1)\n",
    "        t_avg = np.mean(toxicity_scores) # Average toxicity (0..1)\n",
    "\n",
    "        # 4. Final UCQS Formula\n",
    "        # Base 50 + (Sentiment * 40) - (Variance * 20) - (Toxicity * 30)\n",
    "        ucqs = 50 + (s_avg * 40) - (s_var * 20) - (t_avg * 30)\n",
    "\n",
    "        return float(np.clip(ucqs, 0, 100))\n",
    "\n",
    "    def calculate_efficiency(self, score, num_comments, upvote_ratio):\n",
    "        \"\"\"Calculates Adjusted Efficiency Score (Virality).\"\"\"\n",
    "        # Handle null values\n",
    "        score = score if pd.notnull(score) else 0\n",
    "        comments = num_comments if pd.notnull(num_comments) else 0\n",
    "        ratio = upvote_ratio if pd.notnull(upvote_ratio) else 0.5\n",
    "\n",
    "        # Comments are weighted higher (x2) as they require more effort\n",
    "        raw_engagement = score + (comments * 2.0)\n",
    "\n",
    "        # Logarithm smooths out outliers (million-view posts)\n",
    "        log_engagement = np.log1p(raw_engagement)\n",
    "\n",
    "        return round(log_engagement * ratio, 2)"
   ],
   "id": "b64cf62d8f564be3",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T13:40:12.581996Z",
     "start_time": "2025-11-29T13:32:01.719001Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = pd.read_csv(\"/Users/ivantyshchenko/Projects/Python/DataPatron/data/reddit.csv\")\n",
    "metrics_engine = AdvancedMessageMetrics()\n",
    "\n",
    "df['Efficiency'] = df.apply(\n",
    "    lambda x: metrics_engine.calculate_efficiency(\n",
    "        x['score'], x['num_comments'], x['upvote_ratio']\n",
    "    ), axis=1\n",
    ")\n",
    "\n",
    "# 2. Calculate UCQS (this may take time depending on the number of comments)\n",
    "df['UCQS'] = df['comments_content'].apply(metrics_engine.calculate_ucqs)\n"
   ],
   "id": "4cbe5be7cd1266d9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ Loading Sentiment Model (XLM-RoBERTa)...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'endswith'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mAttributeError\u001B[39m                            Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[8]\u001B[39m\u001B[32m, line 2\u001B[39m\n\u001B[32m      1\u001B[39m df = pd.read_csv(\u001B[33m\"\u001B[39m\u001B[33m/Users/ivantyshchenko/Projects/Python/DataPatron/data/reddit.csv\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m----> \u001B[39m\u001B[32m2\u001B[39m metrics_engine = \u001B[43mAdvancedMessageMetrics\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m      4\u001B[39m df[\u001B[33m'\u001B[39m\u001B[33mEfficiency\u001B[39m\u001B[33m'\u001B[39m] = df.apply(\n\u001B[32m      5\u001B[39m     \u001B[38;5;28;01mlambda\u001B[39;00m x: metrics_engine.calculate_efficiency(\n\u001B[32m      6\u001B[39m         x[\u001B[33m'\u001B[39m\u001B[33mscore\u001B[39m\u001B[33m'\u001B[39m], x[\u001B[33m'\u001B[39m\u001B[33mnum_comments\u001B[39m\u001B[33m'\u001B[39m], x[\u001B[33m'\u001B[39m\u001B[33mupvote_ratio\u001B[39m\u001B[33m'\u001B[39m]\n\u001B[32m      7\u001B[39m     ), axis=\u001B[32m1\u001B[39m\n\u001B[32m      8\u001B[39m )\n\u001B[32m     10\u001B[39m \u001B[38;5;66;03m# 2. Calculate UCQS (this may take time depending on the number of comments)\u001B[39;00m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[7]\u001B[39m\u001B[32m, line 5\u001B[39m, in \u001B[36mAdvancedMessageMetrics.__init__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m      3\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33m⏳ Loading Sentiment Model (XLM-RoBERTa)...\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m      4\u001B[39m \u001B[38;5;66;03m# Using CPU (device=-1). If you have a GPU, change to device=0\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m5\u001B[39m \u001B[38;5;28mself\u001B[39m.sentiment_pipe = \u001B[43mpipeline\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m      6\u001B[39m \u001B[43m    \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43msentiment-analysis\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m      7\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mcardiffnlp/twitter-xlm-roberta-base-sentiment\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m      8\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mcardiffnlp/twitter-xlm-roberta-base-sentiment\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m      9\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtop_k\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m     10\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtruncation\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m     11\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmax_length\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m512\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     12\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mmps\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\n\u001B[32m     13\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     15\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33m⏳ Loading Toxicity Model (Multilingual Toxic XLM-R)...\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m     16\u001B[39m \u001B[38;5;28mself\u001B[39m.toxic_pipe = pipeline(\n\u001B[32m     17\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mtext-classification\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m     18\u001B[39m     model=\u001B[33m\"\u001B[39m\u001B[33munitary/multilingual-toxic-xlm-roberta\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m     23\u001B[39m     device=-\u001B[32m1\u001B[39m\n\u001B[32m     24\u001B[39m )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Projects/Python/DataPatron/.venv/lib/python3.12/site-packages/transformers/pipelines/__init__.py:1078\u001B[39m, in \u001B[36mpipeline\u001B[39m\u001B[34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001B[39m\n\u001B[32m   1076\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m   1077\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m load_tokenizer:\n\u001B[32m-> \u001B[39m\u001B[32m1078\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[32m   1079\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1080\u001B[39m         tokenizer = \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Projects/Python/DataPatron/.venv/lib/python3.12/site-packages/transformers/pipelines/__init__.py:1073\u001B[39m, in \u001B[36mpipeline\u001B[39m\u001B[34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001B[39m\n\u001B[32m   1070\u001B[39m             tokenizer_kwargs = model_kwargs.copy()\n\u001B[32m   1071\u001B[39m             tokenizer_kwargs.pop(\u001B[33m\"\u001B[39m\u001B[33mtorch_dtype\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m), tokenizer_kwargs.pop(\u001B[33m\"\u001B[39m\u001B[33mdtype\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[32m-> \u001B[39m\u001B[32m1073\u001B[39m         tokenizer = \u001B[43mAutoTokenizer\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1074\u001B[39m \u001B[43m            \u001B[49m\u001B[43mtokenizer_identifier\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43muse_fast\u001B[49m\u001B[43m=\u001B[49m\u001B[43muse_fast\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m_from_pipeline\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mhub_kwargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mtokenizer_kwargs\u001B[49m\n\u001B[32m   1075\u001B[39m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1076\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m   1077\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m load_tokenizer:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Projects/Python/DataPatron/.venv/lib/python3.12/site-packages/transformers/models/auto/tokenization_auto.py:1175\u001B[39m, in \u001B[36mAutoTokenizer.from_pretrained\u001B[39m\u001B[34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001B[39m\n\u001B[32m   1172\u001B[39m tokenizer_class_py, tokenizer_class_fast = TOKENIZER_MAPPING[\u001B[38;5;28mtype\u001B[39m(config)]\n\u001B[32m   1174\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m tokenizer_class_fast \u001B[38;5;129;01mand\u001B[39;00m (use_fast \u001B[38;5;129;01mor\u001B[39;00m tokenizer_class_py \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[32m-> \u001B[39m\u001B[32m1175\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtokenizer_class_fast\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpretrained_model_name_or_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1176\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1177\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m tokenizer_class_py \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Projects/Python/DataPatron/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2113\u001B[39m, in \u001B[36mPreTrainedTokenizerBase.from_pretrained\u001B[39m\u001B[34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001B[39m\n\u001B[32m   2110\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   2111\u001B[39m         logger.info(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mloading file \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfile_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m from cache at \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresolved_vocab_files[file_id]\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m-> \u001B[39m\u001B[32m2113\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mcls\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_from_pretrained\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   2114\u001B[39m \u001B[43m    \u001B[49m\u001B[43mresolved_vocab_files\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2115\u001B[39m \u001B[43m    \u001B[49m\u001B[43mpretrained_model_name_or_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2116\u001B[39m \u001B[43m    \u001B[49m\u001B[43minit_configuration\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2117\u001B[39m \u001B[43m    \u001B[49m\u001B[43m*\u001B[49m\u001B[43minit_inputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2118\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtoken\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtoken\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2119\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2120\u001B[39m \u001B[43m    \u001B[49m\u001B[43mlocal_files_only\u001B[49m\u001B[43m=\u001B[49m\u001B[43mlocal_files_only\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2121\u001B[39m \u001B[43m    \u001B[49m\u001B[43m_commit_hash\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcommit_hash\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2122\u001B[39m \u001B[43m    \u001B[49m\u001B[43m_is_local\u001B[49m\u001B[43m=\u001B[49m\u001B[43mis_local\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2123\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtrust_remote_code\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtrust_remote_code\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2124\u001B[39m \u001B[43m    \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2125\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Projects/Python/DataPatron/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2359\u001B[39m, in \u001B[36mPreTrainedTokenizerBase._from_pretrained\u001B[39m\u001B[34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001B[39m\n\u001B[32m   2357\u001B[39m \u001B[38;5;66;03m# Instantiate the tokenizer.\u001B[39;00m\n\u001B[32m   2358\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m2359\u001B[39m     tokenizer = \u001B[38;5;28;43mcls\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43minit_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43minit_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   2360\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m import_protobuf_decode_error():\n\u001B[32m   2361\u001B[39m     logger.info(\n\u001B[32m   2362\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mUnable to load tokenizer model from SPM, loading from TikToken will be attempted instead.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   2363\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33m(Google protobuf error: Tried to load SPM model with non-SPM vocab file).\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m   2364\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Projects/Python/DataPatron/.venv/lib/python3.12/site-packages/transformers/models/xlm_roberta/tokenization_xlm_roberta_fast.py:108\u001B[39m, in \u001B[36mXLMRobertaTokenizerFast.__init__\u001B[39m\u001B[34m(self, vocab_file, tokenizer_file, bos_token, eos_token, sep_token, cls_token, unk_token, pad_token, mask_token, **kwargs)\u001B[39m\n\u001B[32m     92\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__init__\u001B[39m(\n\u001B[32m     93\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m     94\u001B[39m     vocab_file=\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m   (...)\u001B[39m\u001B[32m    104\u001B[39m ):\n\u001B[32m    105\u001B[39m     \u001B[38;5;66;03m# Mask token behave like a normal word, i.e. include the space before it\u001B[39;00m\n\u001B[32m    106\u001B[39m     mask_token = AddedToken(mask_token, lstrip=\u001B[38;5;28;01mTrue\u001B[39;00m, rstrip=\u001B[38;5;28;01mFalse\u001B[39;00m) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(mask_token, \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m mask_token\n\u001B[32m--> \u001B[39m\u001B[32m108\u001B[39m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[34;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[32m    109\u001B[39m \u001B[43m        \u001B[49m\u001B[43mvocab_file\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    110\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtokenizer_file\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtokenizer_file\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    111\u001B[39m \u001B[43m        \u001B[49m\u001B[43mbos_token\u001B[49m\u001B[43m=\u001B[49m\u001B[43mbos_token\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    112\u001B[39m \u001B[43m        \u001B[49m\u001B[43meos_token\u001B[49m\u001B[43m=\u001B[49m\u001B[43meos_token\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    113\u001B[39m \u001B[43m        \u001B[49m\u001B[43msep_token\u001B[49m\u001B[43m=\u001B[49m\u001B[43msep_token\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    114\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcls_token\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcls_token\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    115\u001B[39m \u001B[43m        \u001B[49m\u001B[43munk_token\u001B[49m\u001B[43m=\u001B[49m\u001B[43munk_token\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    116\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpad_token\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpad_token\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    117\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmask_token\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmask_token\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    118\u001B[39m \u001B[43m        \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    119\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    121\u001B[39m     \u001B[38;5;28mself\u001B[39m.vocab_file = vocab_file\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Projects/Python/DataPatron/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_fast.py:139\u001B[39m, in \u001B[36mPreTrainedTokenizerFast.__init__\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m    137\u001B[39m     \u001B[38;5;28mself\u001B[39m.vocab_file = kwargs.get(\u001B[33m\"\u001B[39m\u001B[33mvocab_file\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m    138\u001B[39m     \u001B[38;5;28mself\u001B[39m.additional_special_tokens = kwargs.get(\u001B[33m\"\u001B[39m\u001B[33madditional_special_tokens\u001B[39m\u001B[33m\"\u001B[39m, [])\n\u001B[32m--> \u001B[39m\u001B[32m139\u001B[39m     fast_tokenizer = \u001B[43mconvert_slow_tokenizer\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrom_tiktoken\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[32m    140\u001B[39m     slow_tokenizer = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    141\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Projects/Python/DataPatron/.venv/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:1857\u001B[39m, in \u001B[36mconvert_slow_tokenizer\u001B[39m\u001B[34m(transformer_tokenizer, from_tiktoken)\u001B[39m\n\u001B[32m   1855\u001B[39m     converter_class = SLOW_TO_FAST_CONVERTERS[tokenizer_class_name]\n\u001B[32m   1856\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m converter_class(transformer_tokenizer).converted()\n\u001B[32m-> \u001B[39m\u001B[32m1857\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m \u001B[43mtransformer_tokenizer\u001B[49m\u001B[43m.\u001B[49m\u001B[43mvocab_file\u001B[49m\u001B[43m.\u001B[49m\u001B[43mendswith\u001B[49m(\u001B[33m\"\u001B[39m\u001B[33mtekken.json\u001B[39m\u001B[33m\"\u001B[39m):\n\u001B[32m   1858\u001B[39m     transformer_tokenizer.original_tokenizer = transformer_tokenizer\n\u001B[32m   1859\u001B[39m     logger.info(\u001B[33m\"\u001B[39m\u001B[33mConverting from Mistral tekken.json\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[31mAttributeError\u001B[39m: 'NoneType' object has no attribute 'endswith'"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "baab852a383b658c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
