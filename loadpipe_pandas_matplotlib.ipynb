{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# LoadPipe, pandas, and Matplotlib\n",
    "\n",
    "Use this notebook to stream Google Drive payloads through LoadPipe's `DriveFileSystem` and explore them with pandas + Matplotlib. Replace the placeholder file identifiers with IDs from your Drive before running the data-loading cells."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prereqs",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "- Activate your virtual environment and install the extras: ``pip install -e loadpipe[extras,gdrive] pandas matplotlib pyarrow``.\n",
    "- Run ``lp auth login`` once so the OAuth token under ``.secrets/`` is ready for programmatic access.\n",
    "- Update ``loadpipe/configs/config.yaml`` (or point ``CONFIG_PATH`` below to your custom file) with cache paths, Drive folder IDs, and chunk sizes."
   ]
  },
  {
   "cell_type": "code",
   "id": "imports",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T11:06:42.508456Z",
     "start_time": "2025-11-08T11:06:41.565881Z"
    }
   },
   "source": [
    "from pathlib import Path\n",
    "import io\n",
    "\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from loadpipe.config import Config\n",
    "from loadpipe.filesystem import DriveFileSystem, filesystem_from_config\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use(\"seaborn-v0_8\")\n",
    "\n",
    "# Build a DriveFileSystem wired to your config for reuse across the notebook.\n",
    "CONFIG_PATH = Path(\"/Users/ivantyshchenko/Projects/Python/DataPatron/loadpipe/configs/config.yaml\")\n",
    "cfg = Config.from_file(CONFIG_PATH)\n",
    "drive_fs: DriveFileSystem = filesystem_from_config(cfg)\n",
    "\n",
    "cfg\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Config(runtime=RuntimeConfig(cache_dir='.cache/loadpipe', state_db='.state/manifest.sqlite', cache_limit_gb=30, retries=5, log_dir='.logs'), auth=AuthConfig(client_secrets_path='.secrets/client_secrets.json', token_path='.secrets/token.json', scopes=['https://www.googleapis.com/auth/drive']), source=SourceConfig(folder_id='CHANGE_ME_SOURCE_FOLDER_ID', pattern='*.zst'), download=DownloadConfig(chunk_mb=64), process=ProcessConfig(kind='identity'), upload=UploadConfig(folder_id='CHANGE_ME_DEST_FOLDER_ID', name_suffix=''))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "helpers-title",
   "metadata": {},
   "source": [
    "## Helper utilities\n",
    "These helpers wrap the `DriveFileSystem` so you can call them with a Drive file ID whenever you need a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "id": "helpers",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T11:06:47.031013Z",
     "start_time": "2025-11-08T11:06:47.026693Z"
    }
   },
   "source": [
    "from typing import Any, Optional\n",
    "\n",
    "def read_csv_from_drive(\n",
    "    file_id: str,\n",
    "    *,\n",
    "    fs: DriveFileSystem,\n",
    "    encoding: str = \"utf-8\",\n",
    "    **read_csv_kwargs: Any,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Download a CSV from Drive and materialize it as a DataFrame.\"\"\"\n",
    "    if not file_id:\n",
    "        raise ValueError(\"file_id must be provided.\")\n",
    "    drive_url = f\"gdrive://{file_id}\"\n",
    "    with fs.open(drive_url) as binary_stream:\n",
    "        with io.TextIOWrapper(binary_stream, encoding=encoding) as text_stream:\n",
    "            return pd.read_csv(text_stream, **read_csv_kwargs)\n",
    "\n",
    "def read_parquet_from_drive(\n",
    "    file_id: str,\n",
    "    *,\n",
    "    fs: DriveFileSystem,\n",
    "    columns: Optional[list[str]] = None,\n",
    "    **read_parquet_kwargs: Any,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Random-access reader for Parquet/Arrow workloads.\"\"\"\n",
    "    if not file_id:\n",
    "        raise ValueError(\"file_id must be provided.\")\n",
    "    drive_url = f\"gdrive://{file_id}\"\n",
    "    with fs.open(drive_url, random_access=True) as ra_stream:\n",
    "        return pd.read_parquet(ra_stream, columns=columns, **read_parquet_kwargs)\n"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "csv-title",
   "metadata": {},
   "source": [
    "## Load a CSV into pandas\n",
    "Provide a Drive file ID that points to a CSV export (for example, a daily metrics dump or a partner feed). Uncomment `parse_dates` or dtype hints to match your schema."
   ]
  },
  {
   "cell_type": "code",
   "id": "csv-load",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T11:06:51.948248Z",
     "start_time": "2025-11-08T11:06:51.944527Z"
    }
   },
   "source": [
    "CSV_FILE_ID = \"\"  # e.g. \"1AbCdEfGhIj...\"\n",
    "\n",
    "if CSV_FILE_ID:\n",
    "    orders_df = read_csv_from_drive(\n",
    "        CSV_FILE_ID,\n",
    "        fs=drive_fs,\n",
    "        # parse_dates=[\"event_ts\"],  # uncomment if you have timestamps\n",
    "        # dtype={\"partner_id\": \"string\"},\n",
    "    )\n",
    "    display(orders_df.head())\n",
    "    print(f\"Loaded {len(orders_df):,} rows and {len(orders_df.columns)} columns.\")\n",
    "else:\n",
    "    print(\"Set CSV_FILE_ID to a Drive file id before running this cell.\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set CSV_FILE_ID to a Drive file id before running this cell.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "profile-title",
   "metadata": {},
   "source": [
    "### Quick profiling\n",
    "Once the DataFrame is in memory you can lean on pandas to inspect schema, basic statistics, and null coverage."
   ]
  },
  {
   "cell_type": "code",
   "id": "profile",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T15:30:51.322272Z",
     "start_time": "2025-11-07T15:30:51.318603Z"
    }
   },
   "source": [
    "if \"orders_df\" in globals():\n",
    "    orders_df.info()\n",
    "    summary = orders_df.describe(include=\"all\").transpose()\n",
    "    display(summary)\n",
    "else:\n",
    "    print(\"Load a CSV DataFrame first by setting CSV_FILE_ID.\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load a CSV DataFrame first by setting CSV_FILE_ID.\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "id": "plot-title",
   "metadata": {},
   "source": [
    "### Plotting with Matplotlib\n",
    "Pick a numeric column (and optional grouping key) to visualize. If you leave `GROUP_BY_COLUMN` blank the plot shows the raw series across the row index."
   ]
  },
  {
   "cell_type": "code",
   "id": "plot",
   "metadata": {},
   "source": [
    "COLUMN_TO_PLOT = \"\"  # e.g. \"bytes_transferred\"\n",
    "GROUP_BY_COLUMN = \"\"  # e.g. \"region\"\n",
    "\n",
    "if \"orders_df\" not in globals():\n",
    "    print(\"Load orders_df first.\")\n",
    "elif not COLUMN_TO_PLOT:\n",
    "    print(\"Set COLUMN_TO_PLOT to a numeric column before plotting.\")\n",
    "elif COLUMN_TO_PLOT not in orders_df.columns:\n",
    "    print(f\"Column {COLUMN_TO_PLOT!r} not found. Available columns: {orders_df.columns.tolist()}\")\n",
    "elif GROUP_BY_COLUMN and GROUP_BY_COLUMN not in orders_df.columns:\n",
    "    print(f\"Grouping column {GROUP_BY_COLUMN!r} not found. Available columns: {orders_df.columns.tolist()}\")\n",
    "else:\n",
    "    fig, ax = plt.subplots(figsize=(9, 4))\n",
    "    if GROUP_BY_COLUMN:\n",
    "        series = (\n",
    "            orders_df.groupby(GROUP_BY_COLUMN)[COLUMN_TO_PLOT]\n",
    "            .mean()\n",
    "            .sort_values(ascending=False)\n",
    "            .head(15)\n",
    "        )\n",
    "        series.plot(kind=\"barh\", ax=ax, color=\"#4c72b0\")\n",
    "        ax.set_xlabel(f\"Mean {COLUMN_TO_PLOT}\")\n",
    "        ax.set_ylabel(GROUP_BY_COLUMN)\n",
    "        ax.set_title(f\"Top {len(series)} {GROUP_BY_COLUMN} groups by {COLUMN_TO_PLOT}\")\n",
    "    else:\n",
    "        orders_df[COLUMN_TO_PLOT].plot(ax=ax, color=\"#4c72b0\")\n",
    "        ax.set_ylabel(COLUMN_TO_PLOT)\n",
    "        ax.set_title(f\"{COLUMN_TO_PLOT} across the loaded rows\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "parquet-title",
   "metadata": {},
   "source": [
    "## Reading Parquet/Arrow files\n",
    "Parquet readers need random access. LoadPipe exposes that via `fs.open(..., random_access=True)`, which the helper above already handles. Make sure ``pyarrow`` is installed."
   ]
  },
  {
   "cell_type": "code",
   "id": "parquet",
   "metadata": {},
   "source": [
    "PARQUET_FILE_ID = \"\"\n",
    "\n",
    "if PARQUET_FILE_ID:\n",
    "    events_df = read_parquet_from_drive(\n",
    "        PARQUET_FILE_ID,\n",
    "        fs=drive_fs,\n",
    "        # columns=[\"event_ts\", \"bytes_transferred\"],\n",
    "    )\n",
    "    display(events_df.head())\n",
    "    print(f\"Loaded {len(events_df):,} parquet rows.\")\n",
    "else:\n",
    "    print(\"Set PARQUET_FILE_ID to a Drive file id before running this cell.\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "tips",
   "metadata": {},
   "source": [
    "## Tips\n",
    "- `cfg.runtime.cache_dir` stores downloaded bytes so rerunning cells only hits Drive when the cache is cold.\n",
    "- Use `drive_fs.open(..., random_access=True)` whenever a consumer seeks (Parquet/Arrow, DuckDB, Polars, etc.).\n",
    "- Keep secrets in `.secrets/` and rely on `Config.from_file` or environment overrides instead of embedding credentials in notebooks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
